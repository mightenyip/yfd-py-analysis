name: Monday Morning Scraper (Sunday Games)

# üìä EXPECTED RESULTS (based on Week 6 manual scraper success):
# - Raw scraped data: ~700-800 players (with duplicates across game times)
# - After deduplication: ~400 unique players
# - Position breakdown: QB(~55-60), RB(~90-95), WR(~135-140), TE(~80-85), DEF(~25-30)
# - File output: weekX_Sunday_all_games.csv (comprehensive data with duplicates)
# - Note: Duplicates occur because same players appear in multiple game time slates
#   Example: Patrick Mahomes appears in both "1:00 PM (10 NFL Games)" and "1:00 PM (7 NFL Games)"

on:
  schedule:
    # Runs every Monday at 8:00 AM PST (16:00 UTC)
    - cron: '0 16 * * 1'
  
  # Allow manual trigger from GitHub Actions tab
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install selenium pandas beautifulsoup4 requests matplotlib seaborn numpy
      
      - name: Install ChromeDriver
        run: |
          sudo apt-get update
          sudo apt-get install -y chromium-browser chromium-chromedriver
          sudo ln -s /usr/lib/chromium-browser/chromedriver /usr/local/bin/chromedriver
      
      - name: Run Monday scraper
        run: |
          echo "Starting Monday morning scraper..."
          python3 scrapers/automated_yahoo_scraper.py --headless --week $(python3 -c "from datetime import datetime; start = datetime(2025, 9, 4); print((datetime.now() - start).days // 7 + 1)") --day Sunday || echo "Yahoo scraper completed with warnings"
      
      - name: Verify scraping results
        run: |
          echo "üîç Verifying scraping results..."
          WEEK=$(python3 -c "from datetime import datetime; start = datetime(2025, 9, 4); print((datetime.now() - start).days // 7 + 1)")
          FILE="data_csv/week${WEEK}_Sunday_all_games.csv"
          
          if [ -f "$FILE" ]; then
            TOTAL_ROWS=$(wc -l < "$FILE")
            PLAYER_COUNT=$((TOTAL_ROWS - 1))  # Subtract header row
            
            echo "üìä Scraping Results Summary:"
            echo "  - File: $FILE"
            echo "  - Total rows: $TOTAL_ROWS"
            echo "  - Players scraped: $PLAYER_COUNT"
            
            if [ $PLAYER_COUNT -ge 600 ]; then
              echo "  ‚úÖ SUCCESS: Comprehensive data captured (~$PLAYER_COUNT players)"
              echo "  üìù Expected after deduplication: ~400 unique players"
            elif [ $PLAYER_COUNT -ge 300 ]; then
              echo "  ‚ö†Ô∏è  PARTIAL: Some data captured ($PLAYER_COUNT players)"
              echo "  üìù May need manual verification"
            else
              echo "  ‚ùå ISSUE: Low player count ($PLAYER_COUNT players)"
              echo "  üìù May need manual scraping"
            fi
            
            # Show position breakdown if possible
            if command -v python3 >/dev/null 2>&1; then
              echo ""
              echo "üìà Position breakdown:"
              python3 << 'EOF'
import pandas as pd
try:
    df = pd.read_csv('$FILE')
    if 'position' in df.columns:
        print(df['position'].value_counts().sort_index().to_string())
    elif 'Position' in df.columns:
        print(df['Position'].value_counts().sort_index().to_string())
    else:
        print('Position column not found')
except Exception as e:
    print(f'Could not analyze positions: {e}')
EOF
            fi
          else
            echo "‚ùå ERROR: Expected file $FILE not found"
            echo "üìù Scraping may have failed"
          fi
      
      - name: Commit and push changes
        run: |
          git config --global user.name "GitHub Actions Bot"
          git config --global user.email "actions@github.com"
          git add -A
          
          # Only commit if there are changes
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            WEEK=$(python3 -c "from datetime import datetime; start = datetime(2025, 9, 4); print((datetime.now() - start).days // 7 + 1)")
            git commit -m "ü§ñ Automated Monday scrape: Week $WEEK Sunday data - $(date +%Y-%m-%d)"
            git push
            echo "Changes pushed successfully!"
          fi
      
      - name: Upload logs as artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: monday-logs-${{ github.run_number }}
          path: logs/
          retention-days: 30

